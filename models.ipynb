{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallownet_sequential(width, height, depth, classes):\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" ordering\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    # define the first (and only) CONV => RELU layer\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    # return the constructed network architecture\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigooglenet_functional(width,height,depth,classes):\n",
    "    '''\n",
    "    function: conv_module\n",
    "    parameters:\n",
    "    x=input parameter\n",
    "    K=filters\n",
    "    kX,kY=Kernal_size\n",
    "    stride=strides\n",
    "    chanDim=channels\n",
    "    \n",
    "    syntax : keras.layers.Conv2D(filters, kernel_size, strides=(1, 1),padding='valid')\n",
    "    '''\n",
    "    def conv_module(x,K,kX,kY,stride,chanDim,padding='same'):\n",
    "        #defining a CONV => BN => RELU pattern\n",
    "        x=Conv2D(K,(kX,kY),strides=stride,padding=padding)(x)\n",
    "        x=BatchNormalization(axis=chanDim)(x)\n",
    "        x=Activation('relu')(x)\n",
    "        \n",
    "        #Return the block\n",
    "        return x\n",
    "    \n",
    "    '''\n",
    "    function: inception_module\n",
    "    parameters:\n",
    "    x=input parameter\n",
    "    numK1x1 = 1 X 1 filter\n",
    "    numK3X3 = 3 X 3 filter\n",
    "    '''\n",
    "    \n",
    "    def inception_module(x,numK1x1,numK3x3,chanDim):\n",
    "        #define two CONV modules, then concatenate across the channel dimension\n",
    "        conv_1x1=conv_module(x,numK1x1,1,1,(1,1),chanDim)#performs 1X1 convolutions\n",
    "        conv_3x3=conv_module(x,numK3x3,3,3,(1,1),chanDim)#performs 3X3 convolutions\n",
    "        x=concatenate([conv_1x1,conv_3x3],axis=chanDim)\n",
    "        \n",
    "        #return the block\n",
    "        return x\n",
    "    \n",
    "    #downsample_module is responsible for reducing the input volume size \n",
    "    def downsample_module(x,K,chanDim):\n",
    "        #define the CONV module and POOL, then concatenate\n",
    "        #across the channel dimensions\n",
    "        conv_3x3=conv_module(x,K,3,3,(2,2),chanDim,padding='valid')\n",
    "        pool=MaxPooling2D((3,3),strides=(2,2))(x)\n",
    "        c=concatenate([conv_3x3,pool],axis=chanDim)\n",
    "        \n",
    "        #return the block\n",
    "        return x\n",
    "    \n",
    "    #initialize the input shape to be 'channels last' and the channels dimension itself\n",
    "    inputShape=(height,width,depth)\n",
    "    chanDim=-1\n",
    "    \n",
    "    #define the model input and first CONV module\n",
    "    inputs=Input(shape=inputShape)\n",
    "    x=conv_module(inputs,96,3,3,(1,1),chanDim)\n",
    "    \n",
    "    #two inception modules followed by a downsample module\n",
    "    x=inception_module(x,32,32,chanDim)\n",
    "    x=inception_module(x,32,48,chanDim)\n",
    "    x=downsample_module(x,80,chanDim)\n",
    "    \n",
    "    #four inception modules followed by a downsample module\n",
    "    x=inception_module(x,112,48,chanDim)\n",
    "    x=inception_module(x,96,64,chanDim)\n",
    "    x=inception_module(x,80,80,chanDim)\n",
    "    x=inception_module(x,48,96,chanDim)\n",
    "    x=downsample_module(x,96,chanDim)\n",
    "    \n",
    "    #twoo inception modules followed by global POOL and dropout layer\n",
    "    x=inception_module(x,176,160,chanDim)\n",
    "    x=inception_module(x,176,160,chanDim)\n",
    "    x=AveragePooling2D((7,7))(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    \n",
    "    #softmax classifier\n",
    "    x=Flatten()(x)\n",
    "    X=Dense(classes)(x)\n",
    "    x=Activation('softmax')(x)\n",
    "    \n",
    "    #create the model\n",
    "    model=Model(inputs,x,name='MiniGoogLeNet')\n",
    "    \n",
    "    #Return the constructed network architecture\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGGNetModel(Model):\n",
    "    def __init__(self,classes,chanDim=-1):\n",
    "        #calling parent constructor\n",
    "        super(MiniVGGNetModel,self).__init__()\n",
    "        \n",
    "        #intialize the layers in the first (CONV => RELU) * 2 => POOL\n",
    "        #layer set\n",
    "        self.conv1A=Conv2D(32,(3,3),padding='same')\n",
    "        self.act1A=Activation('relu')\n",
    "        self.bn1A=BatchNormalization(axis=chanDim)\n",
    "        self.conv1B=Conv2D(32,(3,3),padding='same')\n",
    "        self.act1B=Activation('relu')\n",
    "        self.bn1B=BatchNormalization(axis=chanDim)\n",
    "        self.pool1=MaxPooling2D(pool_size=(2,2))\n",
    "        \n",
    "        #intialize the layers in the second (CONV => RELU) * 2 => POOL\n",
    "        self.conv2A=Conv2D(32,(3,3),padding='same')\n",
    "        self.act2A=Activation('relu')\n",
    "        self.bn2A=BatchNormalization(axis=chanDim)\n",
    "        self.conv2B=Conv2D(32,(3,3),padding='same')\n",
    "        self.act2B=Activation('relu')\n",
    "        self.bn2B=BatchNormalization(axis=chanDim)\n",
    "        self.pool2=MaxPooling2D(pool_size=(2,2))\n",
    "        \n",
    "        #intialize the layers in our fully connected layer set\n",
    "        self.flatten=Flatten()\n",
    "        self.dense3=Dense(512)\n",
    "        self.act3=Activation('relu')\n",
    "        self.bn3=BatchNormalization()\n",
    "        self.do3=Dropout(0.5)\n",
    "        \n",
    "        #intialize the layers in the softmax classifier layer set\n",
    "        self.dense4=Dense(classes)\n",
    "        self.softmax=Activation('softmax')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        #Build the first (CONV => RELU) * 2 => POOl layer set\n",
    "        x=self.conv1A(inputs)\n",
    "        x=self.act1A(x)\n",
    "        x=self.bn1A(x)\n",
    "        x=self.conv1B(x)\n",
    "        x=self.act1B(x)\n",
    "        x=self.bn1B(x)\n",
    "        x=self.pool1(x)\n",
    "\n",
    "        #Build the second (CONV => RELU) * 2 => POOl layer set\n",
    "        x=self.conv2A(inputs)\n",
    "        x=self.act2A(x)\n",
    "        x=self.bn2A(x)\n",
    "        x=self.conv2B(x)\n",
    "        x=self.act2B(x)\n",
    "        x=self.bn2B(x)\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        #Build fully connected layer set\n",
    "        x=self.flatten(x)\n",
    "        x=self.dense3(x)\n",
    "        x=self.act3(x)\n",
    "        x=self.bn3(x)\n",
    "        x=self.do3(x)\n",
    "\n",
    "        #Build the softmax classifier\n",
    "        x=self.dense4(x)\n",
    "        x=self.softmax(x)\n",
    "\n",
    "        #Return the constructed model\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer here is defined inside the constructor purposely.\n",
    "\n",
    "Once our keras layers and custom implemented layers are defined , we can then define the network topology/graph inside the call fuction.Which is used to perform a forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
